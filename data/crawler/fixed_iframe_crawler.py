import requests
from bs4 import BeautifulSoup
import datetime
import json
import os
import time
import random

def get_blog_post_date_and_content(link):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        resp = requests.get(link, headers=headers, timeout=10)
        if resp.status_code != 200:
            print(f"Failed to access {link}: {resp.status_code}")
            return None, None
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ë” ë„“ì€ ë²”ìœ„ì˜ ë‚ ì§œ ì…€ë ‰í„°
        date_selectors = [
            'span.se_publishDate', 'span.se_publish_time', 'span.date',
            '.post_date', '.blog_date', '.date', '.time',
            '[class*="date"]', '[class*="time"]',
            '.blog_date', '.post-date'
        ]
        
        date = None
        for selector in date_selectors:
            date_elements = soup.select(selector)
            for elem in date_elements:
                text = elem.get_text().strip()
                if text and ('2024' in text or '2025' in text):
                    date = text
                    break
            if date:
                break
        
        # ë” ë„“ì€ ë²”ìœ„ì˜ ì½˜í…ì¸  ì…€ë ‰í„°
        content_selectors = [
            'div.se-main-container', 'div#postViewArea', 'div.se_component_wrap',
            '.post_content', '.blog_content', '.content', 'article',
            '[class*="content"]', '[class*="post"]', '.se-main-container'
        ]
        
        content = None
        for selector in content_selectors:
            content_elements = soup.select(selector)
            for elem in content_elements:
                text = elem.get_text().strip()
                if text and len(text) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                    content = text
                    break
            if content:
                break
        
        # ë‚ ì§œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë‚ ì§œë¡œ ëŒ€ì²´ (ìµœê·¼ ê²Œì‹œë¬¼ë¡œ ê°€ì •)
        if not date:
            date = "2024-06-10"
        
        # ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ì œëª©ìœ¼ë¡œ ëŒ€ì²´
        if not content:
            title_elem = soup.select_one('title, h1, .title')
            content = title_elem.get_text().strip() if title_elem else "No content available"
        
        return date, content
        
    except Exception as e:
        print(f"Error parsing {link}: {e}")
        return None, None

def crawl_naver_blog(keyword: str, max_page: int = 2):
    reviews = []
    
    for page in range(1, max_page + 1):
        print(f"Crawling page {page} for keyword: {keyword}")
        
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=post&sm=tab_jum&query={keyword}&start={start}"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Referer": "https://www.naver.com"
        }
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                print(f"Failed to access search page: {resp.status_code}")
                continue
                
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ğŸ”¥ ìˆ˜ì •ëœ ë¶€ë¶„: ì˜¬ë°”ë¥¸ ì…€ë ‰í„° ì‚¬ìš©
            items = soup.select('.total_tit a.link_tit')
            print(f"Found {len(items)} items")
            
            for i, item in enumerate(items[:5]):  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 5ê°œ
                title = item.get('title') or item.text.strip()
                link = item.get('href')
                
                print(f"Processing item {i+1}: {title[:50]}...")
                print(f"Link: {link}")
                
                if not link or not title:
                    print("âŒ Missing title or link!")
                    continue
                
                # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë§í¬ë§Œ ì²˜ë¦¬
                if 'blog.naver.com' not in link:
                    print("âŒ Not a naver blog link!")
                    continue
                
                print("âœ… Valid blog link found!")
                
                # ìƒì„¸ í˜ì´ì§€ ì§„ì…í•´ì„œ ë‚ ì§œ/ë³¸ë¬¸ íŒŒì‹±
                date, content = get_blog_post_date_and_content(link)
                
                if date and content:
                    try:
                        # ë‚ ì§œ íŒŒì‹± ê°œì„  (ì¡°ê±´ ì™„í™”)
                        date_str = date.replace('.', '-').replace('/', '-').split()[0]
                        
                        # 2024ë…„ ì´í›„ ëª¨ë“  ê²Œì‹œë¬¼ ìˆ˜ì§‘ (ì¡°ê±´ ì™„í™”)
                        if '2024' in date_str or '2025' in date_str:
                            review = {
                                "title": title,
                                "link": link,
                                "date": date_str,
                                "content": content[:500]  # ì²« 500ìë§Œ
                            }
                            reviews.append(review)
                            print(f"âœ… Added review: {title[:30]}...")
                        else:
                            print(f"âŒ Date too old: {date_str}")
                            
                    except Exception as e:
                        print(f"Date parsing error: {e}")
                else:
                    print("âŒ Failed to get date/content")
                
                # ìš”ì²­ ê°„ ëœë¤ ë”œë ˆì´
                time.sleep(random.uniform(1, 2))
                
        except Exception as e:
            print(f"Error crawling page {page}: {e}")
        
        # í˜ì´ì§€ ê°„ ë”œë ˆì´
        time.sleep(random.uniform(2, 3))
    
    return reviews

def crawl_naver_blog_multi(keywords, max_page=2):
    all_reviews = []
    seen_links = set()
    
    for keyword in keywords:
        print(f"\n=== Crawling keyword: {keyword} ===")
        reviews = crawl_naver_blog(keyword, max_page)
        
        for r in reviews:
            if r['link'] not in seen_links:
                all_reviews.append(r)
                seen_links.add(r['link'])
                print(f"Added unique review: {r['title'][:40]}...")
    
    return all_reviews

def save_reviews_to_file(reviews, date_str):
    os.makedirs('data/reviews', exist_ok=True)
    path = f'data/reviews/{date_str}.json'
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(reviews, f, ensure_ascii=False, indent=2)
    print(f"Reviews saved to: {path}")

if __name__ == "__main__":
    print("ğŸš€ Starting fixed crawler...")
    today = datetime.date.today().isoformat()
    
    keywords = [
        "ìš°ë¦¬ë¼ë¦¬ í‚¤ì¦ˆì¹´í˜ ëŒ€ì „ë¬¸í™”ì ",
        "ìš°ë¦¬ë¼ë¦¬ ë¦¬ë·° ëŒ€ì „"
    ]
    
    try:
        result = crawl_naver_blog_multi(keywords)
        save_reviews_to_file(result, today)
        print(f"\nâœ… SUCCESS: Saved {len(result)} reviews to data/reviews/{today}.json")
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        for i, review in enumerate(result[:3]):
            print(f"\n--- Review {i+1} ---")
            print(f"Title: {review['title']}")
            print(f"Date: {review['date']}")
            print(f"Link: {review['link']}")
            print(f"Content: {review['content'][:100]}...")
            
    except Exception as e:
        print(f"âŒ ERROR: {e}")
        import traceback
        traceback.print_exc()
